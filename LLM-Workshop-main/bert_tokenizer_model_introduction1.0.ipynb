{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa29629-0bdb-4f36-93c9-fc9ea075b08b",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe6c7b2-30f5-4a83-a17b-66ceec06980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f2be6a-7f42-4d3a-bd34-5dec9e4a6460",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dccbae7-7e1e-4ae2-b6cf-48a15e766857",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentense= \"In Natural Language Processing (NLP), tokenization is a fundamental step where text is divided into smaller components, known as tokens.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aa22365-25fa-47fe-ab5a-c341a6adb21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1130, 6240, 6828, 18821, 1158, 113, 21239, 2101, 114, 117, 22559, 2734, 1110, 170, 8148, 2585, 1187, 3087, 1110, 3233, 1154, 2964, 5644, 117, 1227, 1112, 22559, 1116, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sentense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bd19d8-fc5e-409d-b006-78c8ef6e2318",
   "metadata": {},
   "source": [
    "input_ids are the most direct representation of the input text in numerical form. During tokenization, each piece of text (e.g., a word or subword) is mapped to a unique integer based on a predefined vocabulary.\n",
    "\n",
    "attention_mask is a binary mask (sequence of 1s and 0s) that specifies which tokens should be attended to and which should not. \n",
    "\n",
    "token_type_ids (also known as segment_ids) are used to distinguish between different parts of the input, especially in tasks that involve multiple input sequences (like question-answering or text-pair classification tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71c3d877-8368-4259-862f-f6d47569c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "token=tokenizer.tokenize(sentense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28a0c814-b6e0-4a1e-a799-c5744fdabe90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bebdd63-6395-4933-9692-61e3f470c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=tokenizer.convert_tokens_to_ids(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7b14500-eadb-4495-99f6-da2383ef913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fa4c54c-8db8-4ac3-8be2-69d5d42ba97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_string=tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9424e5c0-b8fc-43f0-8eed-879f68488eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Natural Language Processing ( NLP ), tokenization is a fundamental step where text is divided into smaller components, known as tokens.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6484bab3-a888-46eb-8960-b7320db34ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=tokenizer.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf56b55-e4eb-4d74-8609-f0c7938a6917",
   "metadata": {},
   "source": [
    "## vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3874d8e9-93a8-4461-a0f5-b86ca92cccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df=pd.DataFrame({\"token\":vocab.keys(),\"token_id\":vocab.values()})\n",
    "vocab_df=vocab_df.sort_values(by='token_id').set_index(\"token_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d132a3a-a08e-4d61-9fdb-3a4509f7c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0adb7a4e-aff9-458a-8671-b2fe7fd92cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id=tokenizer.encode(sentense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "554c762f-2cf2-4520-b2c1-766d72d85b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "215874f9-bd59-41ad-9880-13ebd1d18914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4827f3f5-cb55-40e2-89bc-e66f984db2d9",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acbdce3-4450-4b2d-bfc5-320518b1baee",
   "metadata": {},
   "source": [
    "https://huggingface.co/google-bert/bert-base-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b478f45-cd9e-4642-8c32-657458bccdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel,AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a009f27c-66f3-44a1-93fa-e2c5ea89b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d45ddbe5-3f99-4e7c-9ed3-8dfc0db495ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=BertModel.from_pretrained(model_name)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "044151c5-07a7-4c6a-8a39-a37a4b5e58c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a452bf-edaa-45f3-8a51-602551fb62f0",
   "metadata": {},
   "source": [
    "### Number of encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17c52d96-47dd-463c-9059-0cefaae57429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.encoder.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc213fbe-3ae2-4a61-9f4a-93b71fc499a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d8f97-388c-4984-9f8b-8f4df2cffe0a",
   "metadata": {},
   "source": [
    "### Use Case 1: similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa5281a2-8f2e-47bc-8609-fdab02bfa885",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1=\"Deep learning is a principle within the realm of Artificial Intelligence\"\n",
    "sent2=\"Enhancing one's abilities is solely achieved through learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48acfd7-94ef-4482-8749-a4b298bcb2aa",
   "metadata": {},
   "source": [
    "##### # Tokenizing the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cbf3e2d-5872-4118-b6ff-7fae77424d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = tokenizer.tokenize(sent1)\n",
    "tokens2 = tokenizer.tokenize(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f0dc62ba-4bca-4a46-99a9-22b34b3e61bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deep',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'a',\n",
       " 'principle',\n",
       " 'within',\n",
       " 'the',\n",
       " 'realm',\n",
       " 'of',\n",
       " 'Art',\n",
       " '##ific',\n",
       " '##ial',\n",
       " 'Intelligence']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78838f0e-151d-45fe-aa06-cba456d5c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = tokenizer(sent1, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5072add2-857f-4b39-a849-5c456f2f484e",
   "metadata": {},
   "source": [
    "#### Defining a function to encode the input text and get model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "682df5e0-895b-475d-868f-e700b6dde92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    return model(**encoded_inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee421daf-c4b0-4e74-99dc-765aaca2fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting model predictions for the sentences\n",
    "out1 = predict(sent1)\n",
    "out2 = predict(sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9b708-a7bb-43e5-8222-ae78e449060a",
   "metadata": {},
   "source": [
    "##### Extracting embeddings for the word 'learning' in both sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "abafb594-e6ce-46a1-b996-88ab78d71afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = out1[0:, tokens1.index(\"learning\"), :].detach()[0]\n",
    "emb2 = out2[0:, tokens2.index(\"learning\"), :].detach()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b21cebe-3a45-4eb9-92a2-e784b0b091a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a1bf4df6-fc13-4811-9fb3-df733545c1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.525614857673645"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(emb1,emb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29b3096-740a-4aa3-b81a-19c2a568623c",
   "metadata": {},
   "source": [
    "#### Use Case 2: semantic similarity between two text string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a79a3bbc-6d5b-49db-9623-ac632efefe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f0616070-87c6-4883-bb5b-348bc8ee8c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained(model_name)\n",
    "model=BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "31e8a9db-db44-4cde-9f11-50f70d87b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "### encode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d846dc84-6b44-4d75-8592-61cf35179e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(sent):\n",
    "    inputs_ids=tokenizer(sent,return_tensors='pt',padding=True,truncation=True)\n",
    "    outputs=model(**inputs_ids)\n",
    "    return outputs.last_hidden_state.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b221c7f0-6cef-4fe9-baf5-fe43147f31af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1=\"Deep learning is a principle within the realm of Artificial Intelligence\"\n",
    "sent2=\"Enhancing one's abilities is solely achieved through learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5c2c0b47-632f-446a-ae83-164cc9f9c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = bert_encode(sent1)\n",
    "vec2 = bert_encode(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ec7b7f71-e1d2-4857-94bb-c2d749c139f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec11 = vec1.detach().numpy()\n",
    "vec22=vec2.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f3ad8ab7-040d-4aa8-a061-13d72531b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "079826aa-15d8-412b-9a9b-3a521278ccef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8455614]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vec11,vec22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a970dd45-56b0-4be0-a996-6a2dda8012b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
